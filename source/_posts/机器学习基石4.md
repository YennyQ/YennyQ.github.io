---
title: 机器学习基石4——Feasibility of Learning
date: 2017-10-07 16:02:03
tags: [机器学习, 笔记, 机器学习的可行性]
categories: 机器学习基石
mathjax: true
---
## 4.1 Learning is Impossible?
学习可能是做不到的？

![image](http://ow5t5k2fx.bkt.clouddn.com/4.1.png)

- $ g \approx f $ inside $ D $: sure
- $ g \approx f $ outside $ D $: No

“天下没有免费午饭”哲学启示：'in-sample'可以求得一个最佳的假设$ g $，但是'out-of-sample'，如果没有prefer，假设$ g $和目标$ f $差别可能很远。

## 4.2 Probability to the Rescue
可能的补救

回想统计学中通过抽样样本来推测整个样本情况的问题。

![image](http://ow5t5k2fx.bkt.clouddn.com/4.2.png)

- bin —— assume:
    - orange probability = $ \mu $
    - green probability = $ 1 - \mu $

- sample —— N marbles sampled independently, with
    - orange fraction = $ \nu $
    - green fraction = $ 1 - \nu $

显然$ \nu $已知，那么$ \nu $是否能告诉我们一些'out-of-sample'的$ \mu $的信息呢？
- possibly not: $ \nu $完全不同于$ \mu $
- probably yes: $ \nu $是非常接近$ \mu $的

可以想象，当你抽出的小球数量N越大，$ \nu $大概率越接近$ \mu $。

数学上的支撑：霍夫丁不等式（Hoeffding's Inequality）

$ P[|\nu - \mu| > \epsilon] \le 2 \exp(-2 \epsilon^2N)$

不难看出，右侧$ N $足够大的话，$ \nu $与$ \mu $相差较大的概率是很小的。

所以，**the statement $ \nu = \mu $ is probably approximately correct(PAC)**

## 4.3 Connection to Learning
联系到学习


bin | learning
---|---
未知的橙色小球比例 $ \mu $ | 对于某固定的假设$ h(x) $，输入向量$ x $使得$ h(x) \neq   f(x)$占整个输入空间$ X $的比例
抽样的小球 $ \in $ 整个罐中的小球 | 训练样本 $ x $ $ \in $ 整个数据集$ X $
橙色小球 | $ h $是错误的假设 $ \Leftrightarrow $ $ h(x) \neq f(x)$
绿色小球 | $ h $是正确的假设 $ \Leftrightarrow $ $ h(x) = f(x)$
N个小球样本是从罐中独立随机抽取的 | 检验 $ h $ 的输入样本 $ x $是从数据集 $ D  = \{(x_n, y_n)\}$中独立随机选取的

以上联系4.2节的内容到机器学习，做了类比。得出了以下流程：

![image](http://ow5t5k2fx.bkt.clouddn.com/4.3.png)

也就是说，类似于4.2节统计学中i.i.d抽样推测整个样本概率分布问题，在机器学习中：
for any fixed $ h $, can probably infer unknown $ E_{out}(h) = \epsilon_{x~P} [h(x) \neq f(x)] $ by known $ E_{in}(h) = \frac 1 N \sum_{n=1}^N [h(x) \neq y_n]$.

接下来，对固定假设$ h $，将使用$ E_{in} $表示独立随机抽样样本的错误率，$ E_{out} $表示在整个输入空间中的错误率。

同样，套用霍夫丁不等式，$ P[|E_{in} - E_{out}| > \epsilon] \le 2 \exp(-2 \epsilon^2N)$，只要$ N $足够大，在不需要知道 $ E_{out}$的情况下 ，也可以确保$ E_{in}(h) = E_{out}(h) $ PCA。

但是仅仅在$ E_{in} \approx E_{out} $的情况下，can we claim 'good learning'($ g \approx f$)？
- Yes，如果$ E_{in}(h)$对这个$ h $够小，并且算法$ A $选取了这个$ h $作为$ g $的话。
- No，如果算法选取的$ h $给出的$ E_{in}(h)$如果不是足够小的话。

通过以上讲述，可以看出，相比于学习问题，这反倒更像是一个验证（verification）问题，给出fixed $ h $，确认这个$ h $的表现是不是足够好，对应流程图如下：

![image](http://ow5t5k2fx.bkt.clouddn.com/4.4.png)

## 4.4 Connection to Real Learning
联系到真正的学习

举个例子，丢硬币，150人同时丢五次硬币，其中出现某个人丢出五次全部正面朝上的几率是多少？一个人时几率是$ \frac {1} {32} $，150时这个几率则增大到了$ 1 - (\frac {31} {32})^{150} \gt $ 99%。

![image](http://ow5t5k2fx.bkt.clouddn.com/4.5.png)

从选择角度来说，需要选择错误率最小的，也就是$ E_{in} = 0$，此时抛出的硬币五次全为正面，这一结论却与实际情况相差较大。

为什么会出现这种情况呢？

BAD sample

BAD data: $ E_{in} $ and $ E_{out} $ far away，混入了几个不好的sample，导致该次抽样数据产生出了不好的结果的数据集。

- Bad Data for One $ h $

One | $ D_1$ | $ D_2$ | … | $ D_{1126} $ | … | $ D_{5678} $| … | Hoeffding
---|---|---|---|---|---|---|---|---
 $ h $ | BAD |   |   |   |   | BAD |   | $ P_D[BAD\ D \ for\  h] \le $ …

Hoeffding: small $ P_D[BAD\ D] = \sum_{all\ possible\ D} P(D) \cdot [BAD\ D ]$

- Bad Data for Many $ h $ $ \Leftrightarrow $ no 'freedom of choice' by $ A $ $ \Leftrightarrow $ there exists some $ h $ such that $ E_{out}(h) $ and $ E_{in}(h) $ far away.

Many | $ D_1$ | $ D_2$ | … | $ D_{1126} $ | … | $ D_{5678} $| … | Hoeffding
---|---|---|---|---|---|---|---|---
 $ h_1 $ | BAD |   |   |   |   | BAD |   | $ P_D[BAD\ D \ for\  h_1] \le $ …
 $ h_2 $ |  | BAD |   |   |   |  |   | $ P_D[BAD\ D \ for\  h_2] \le $ …
 $ h_3 $ | BAD | BAD |   |   |   | BAD |   | $ P_D[BAD\ D \ for\  h_3] \le $ …
 … |   |   |   |   |   |   |   | 
 $ h_M $ | BAD |   |   |   |   | BAD |   | $ P_D[BAD\ D \ for\  h_M] \le $ …
 all | BAD | BAD |   |   |   | BAD |   | ?

允许算法自由选择假设$ h $的情况下，就需要避开所有的Bad Data。

计算一下Bound of BAD Data

$ P_D[BAD\ D]$
$ = P_D[BAD \ D\ for\ h_1\ \ or\ \ BAD \ D\ for\ h_2\ \ or\ \ ... \ \ BAD\ D\ for\ h_M]$

$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \le P_D[BAD\ D \ for\  h_1] + P_D[BAD\ D \ for\  h_2] + ... + P_D[BAD\ D \ for\  h_M] $  (union bound)

$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \le 2\exp(-2 \epsilon^2N) + 2 \exp(-2 \epsilon^2N) + ... + 2 \exp(-2 \epsilon^2N) $

$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = 2M\exp(-2 \epsilon^2N)$

因此，机器学习是可以做到的，在下面两个保证下：
- if $ |H| = M $ finite, $ N $ large enough, for whatever $ g $ picked by $ A $, $ E_{out}(g) \approx E_{in}(g) $
- if $ A $ finds one $ g $ with $ E_{in}(g) \approx 0 $, PAC guarantee for $ E_{out}(g) \approx 0 $

![image](http://ow5t5k2fx.bkt.clouddn.com/4.6.png)