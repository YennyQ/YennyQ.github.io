---
title: 机器学习基石2——Learning to Answer Yes or No
tags: [机器学习, 笔记]
categories: 机器学习基石
mathjax: true
---
## 2.1 Perceptron Hypothesis Set

感知器假设空间

依旧以银行核发信用卡为例，讲述Perceptron模型。

- For $ x = (x_1, x_2, \cdots, x_d) $ ——'features of customer', compute a weighted 'score' and 
    - approve credit if $ \sum_{i=1}^d w_i x_i > threshold$
    - deny credit if $ \sum_{i=1}^d w_i x_i < threshold$
- $ y: \{+1(good), -1(bad)\} $, 0 ignored —— linear formula $ h \in H$ are $ h(x) = sign((\sum_{i=1}^d w_i x_i) - threshold) $

如同考试打分，每道题有个特定分数，即权重($w$)，再给定一个及格线，即阈值($threshold$)。

以下仅为了符号方便进行数学简化：

$ h(x) = sign((\sum_{i=1}^d w_i x_i) - threshold) $

$ \ \ \ \ \ \ \ \  = sign((\sum_{i=1}^d w_i x_i) + (-threshold * 1)) $


$ \ \ \ \ \ \ \ \  = sign((\sum_{i=1}^d w_i x_i) + (w_0 * x_0)) $

$ \ \ \ \ \ \ \ \  = sign(\sum_{i=0}^d w_i x_i) $

$ \ \ \ \ \ \ \ \  = sign(\mathbf{w}^T )$

如此，不同的$h(x)$对应不同的向量$\mathbf{w}$，也可以说假设空间$H$即向量$\mathbf{w}$的取值范围。

用直观一点的图像来解释这个感知器假设函数。

为便于观察，特征数量限制在两个，以将图像限定在二维平面内，此时的$ h $函数：$ h(x) = sign(w_0 + w_1 x_1 + w_2 x_2) $

![image](http://ow5t5k2fx.bkt.clouddn.com/2.1.png)

- customer features $ \mathbf{x} $: points on the plane (points in $ R^d $)
- labels $ y $: $ \circ  (+1)$, $ \times (-1)$
- hypothesis $ h $: lines (or hyperplanes in $ R^d $)

所以$ h $是在不同的权值$ \mathbf{w} $下对应的不同的直线（或平面/超平面），因为$ sign $是以0为分界线的函数，此时$ w_0 + w_1 x_1 + w_2 x_2 = 0 $，恰好是这条直线，而直线一侧为正，一侧为负。

**perceptrons <=> linear (binary) classifiers**

## 2.2 Perceptron Learning Algorithm (PLA)

感知器学习算法

Select $ g $ From $ H $

- want: $ g \approx f $(hard when $ f $ unknown)
- almost necessary: $ g \approx f $ on $ D $, ideally $ g(x_n) = f(x_n) = y_n$
- difficult: $ H $ is of infinite size
- idea: start from some $ g_0 $, and 'correct' its mistakes on  $ D $

Perceptron Learning Algorithm

Start from some $ \mathbf{w_0} $ (say, $ \mathbf{0} $), for t = 0, 1, …
- find a mistake of $ \mathbf{w_t} $ called $ (x_{n(t)}, y_{n(t)}) $, $ sign(\mathbf{w_t}^T \mathbf{x_{n(t)}}) \neq y_{n(t)}$
- (try to )correct the mistake by $ \mathbf{w_{t+1}} \leftarrow \mathbf{w_t} + y_{n(t)} \mathbf{x_{n(t)}} $ 

…until no more mistakes
return last $ \mathbf{w} $ (called $ \mathbf{w_{PLA}} $) as g

![image](http://ow5t5k2fx.bkt.clouddn.com/2.2.png)