---
title: 机器学习基石2——Learning to Answer Yes or No
date: 2017-09-18 18:34:03
tags: [机器学习, 笔记, PLA, Pocket]
categories: 机器学习基石
mathjax: true
---
## 2.1 Perceptron Hypothesis Set

感知器假设空间

依旧以银行核发信用卡为例，讲述Perceptron的$ H $。

- For $ x = (x_1, x_2, \cdots, x_d) $ ——'features of customer', compute a weighted 'score' and 
    - approve credit if $ \sum_{i=1}^d w_i x_i > threshold$
    - deny credit if $ \sum_{i=1}^d w_i x_i < threshold$
- $ y: \{+1(good), -1(bad)\} $, 0 ignored —— linear formula $ h \in H$ are $ h(x) = sign((\sum_{i=1}^d w_i x_i) - threshold) $

如同考试打分，每道题有个特定分数，即权重($w$)，再给定一个及格线，即阈值($threshold$)。

以下仅为了符号方便进行数学简化：

$ h(x) = sign((\sum_{i=1}^d w_i x_i) - threshold) $

$ \ \ \ \ \ \ \ \ = sign((\sum_{i=1}^d w_i x_i) + (-threshold * 1)) $

$ \ \ \ \ \ \ \ \ = sign((\sum_{i=1}^d w_i x_i) + (w_0 * x_0)) $

$ \ \ \ \ \ \ \ \ = sign(\sum_{i=0}^d w_i x_i) $

$ \ \ \ \ \ \ \ \ = sign(\mathbf{w}^T \mathbf{x})$

如此，不同的$h(x)$对应不同的向量$\mathbf{w}$，也可以说假设空间$H$即向量$\mathbf{w}$的取值范围。

用直观一点的图像来解释这个感知器假设函数。

为便于观察，特征数量限制在两个，以将图像限定在二维平面内，此时的$ h $函数：$ h(x) = sign(w_0 + w_1 x_1 + w_2 x_2) $

![image](http://ow5t5k2fx.bkt.clouddn.com/2.1.png)

- customer features $ \mathbf{x} $: points on the plane (points in $ R^d $)
- labels $ y $: $ \circ  (+1)$, $ \times (-1)$
- hypothesis $ h $: lines (or hyperplanes in $ R^d $)

所以$ h $是在不同的权值$ \mathbf{w} $下对应的不同的直线（或平面/超平面），因为$ sign $是以0为分界线的函数，此时$ w_0 + w_1 x_1 + w_2 x_2 = 0 $，恰好是这条直线，而直线一侧为正，一侧为负。

**perceptrons $\Leftrightarrow $ linear (binary) classifiers**

## 2.2 Perceptron Learning Algorithm (PLA)

感知器学习算法

Select $ g $ From $ H $

- want: $ g \approx f $(hard when $ f $ unknown)
- almost necessary: $ g \approx f $ on $ D $, ideally $ g(x_n) = f(x_n) = y_n$
- difficult: $ H $ is of infinite size
- idea: start from some $ g_0 $, and 'correct' its mistakes on  $ D $

确定问题点，分析已知条件，列出解题障碍，得出一个大概可能的解法。然后顺着解法讲述了感知器学习算法。

Perceptron Learning Algorithm

Start from some $ \mathbf{w_0} $ (say, $ \mathbf{0} $), for t = 0, 1, …
- find a mistake of $ \mathbf{w_t} $ called $ (x_{n(t)}, y_{n(t)}) $, $ sign(\mathbf{w_t}^T \mathbf{x_{n(t)}}) \neq y_{n(t)}$
- (try to )correct the mistake by $ \mathbf{w_{t+1}} \leftarrow \mathbf{w_t} + y_{n(t)} \mathbf{x_{n(t)}} $ 

…until no more mistakes
return last $ \mathbf{w} $ (called $ \mathbf{w_{PLA}} $) as g

修正公式$ \mathbf{w_{t+1}} = \mathbf{w_t} + y_{n(t)} \mathbf{x_{n(t)}} $怎么来的呢？这里需要用到一点线代：
- 首先，$ sign(\mathbf{w}^T \mathbf{x}) $实际就是向量$ \mathbf{w}  $和向量$ \mathbf{x}  $的内积
- 其次，大家知道垂直向量的内积为0，而内积大于0则两向量夹角小于90°，内积小于0则大于90°
- 分情况考虑，对于一个数据$ (\mathbf{x_n}, y_n) $， $ sign(\mathbf{w}^T \mathbf{x_n}) \neq y_n$无外乎两种情况：
    - $ -1 \neq +1 $，此时向量$ \mathbf{w} $和$ \mathbf{x_n} $夹角大于90°，而$ y_n  $要求正确的$ \mathbf{w}  $应该是与$ \mathbf{x_n}  $小于90°的，为减小$ \mathbf{w} $和$ \mathbf{x_n} $角度，我们就给$ \mathbf{w} + \mathbf{x_n} $
    - $ +1 \neq -1 $，反之同理，为增大角度给$ \mathbf{w} - \mathbf{x_n} $

如下图示

![image](http://ow5t5k2fx.bkt.clouddn.com/2.2.png)

这里有一点需要说明，$ \mathbf{w_{t+1}} = \mathbf{w_t} + y_{n(t)} \mathbf{x_{n(t)}} $ 虽然不能保证就立马能将角度修正为当前$ y_n $需要的角度，但确是一点一点增大/减小的，而随着循环次数的增加，总归是可以到达$ y_n $需要的角度的。

Some Remaining Issues of PLA

- Algorithmic: halt (with no mistake)?
    - naïve cyclic: ??
    - random cyclic: ??
    - other variant: ??
- Learning: $ g \approx f $?
    - on $ D $, if halt, yes (no mistake)
    - outside $ D $: ??
    - if not halting: ??

## 2.3 Guarantee of PLA

PLA算法的可行性保障

### Linear Separability

首先训练样本$ D $要线性可分

![image](http://ow5t5k2fx.bkt.clouddn.com/2.3.png)

### assume linear separable $ D $, does PLA always halt?

证明思路是根据已知条件来求解$ \mathbf{w_t} $和$ \mathbf{w_f} $归一化后的向量内积，越大则两向量越邻近

- PLA Fact: $ \mathbf{w_t} $ Gets More Aligned with $ \mathbf{w_f} $

    linear separable $ D $ $ \Leftrightarrow $ exists perfect $ \mathbf{w_f} $ such that $ y_n = sign( \mathbf{w_f^T} \mathbf{x_n} ) $
    
    - $ \mathbf{w_f} $ perfect hence every $ \mathbf{x_n} $ correctly away from line: 
    
        $ y_{n(t)} \mathbf{w_f^T} \mathbf{x_{n(t)}} \ge \min_n y_n \mathbf{w_f^T} \mathbf{x_n} > 0 $
    
    - $ \mathbf{w_f^T} \mathbf{w_{t+1}} \uparrow $ by updating with any $ (\mathbf{x_{n(t)}}, (\mathbf{y_{n(t)}}) $ 
    
        $ \mathbf{w_f^T} \mathbf{w_{t+1}} = \mathbf{w_f^T}(\mathbf{w_t} + y_{n(t)} \mathbf{x_{n(t)}}) $
        
        $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ge \mathbf{w_f^T}\mathbf{w_t} + \min_n y_n \mathbf{w_f^T} \mathbf{x_n} $
        
        $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ > \mathbf{w_f^T}\mathbf{w_t} + 0 $
    
    $ \mathbf{w_t} $ appears more aligned with $ \mathbf{w_f} $ after update ?? NO, 还需排除长度的因素
    
- PLA Fact: $ \mathbf{w_t} $ Does Not Grow Too Fast
   
    $ \mathbf{w_t} $ changed only when mistake $ \Leftrightarrow $ $ sign( \mathbf{w_t^T} \mathbf{x_{n(t)}} ) \neq y_{n(t)} $ $ \Leftrightarrow $ $ y_{n(t)} \mathbf{w_t^T} \mathbf{x_{n(t)}} \le 0$

    - mistake 'limits' $ ||\mathbf{w_t}||^2 $ growth, even when updating with 'longest' $ \mathbf{x_n} $
    
        $ ||\mathbf{w_{t+1}}||^2 = ||\mathbf{w_t} + y_{n(t)} \mathbf{x_{n(t)}} ||^2$
        
        $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ||\mathbf{w_t}||^2 + 2y_{n(t)} \mathbf{w_t^T} \mathbf{x_{n(t)}} + ||y_{n(t)} \mathbf{x_{n(t)}} ||^2$
        
        $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \le ||\mathbf{w_t}||^2 + 0 + ||y_{n(t)} \mathbf{x_{n(t)}} ||^2$
        
        $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \le ||\mathbf{w_t}||^2 + \max_n||y_n \mathbf{x_n} ||^2$
        
        $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ||\mathbf{w_t}||^2 + \max_n|| \mathbf{x_n} ||^2$
    
    start from $ \mathbf{w_0} = 0 $, after $ T $ mistake corrections,
        $ \frac {\mathbf{w_f^T}} {||\mathbf{w_f}||} \frac {\mathbf{w_T}} {||\mathbf{w_T}||} \ge \sqrt{T} \cdot C $
        
### 以下简单计算常数 $ C $以及 $ T $的上界

$ \mathbf{w_f^T} \mathbf{w_T} = \mathbf{w_f^T} (\mathbf{w_{T-1}} + y_n \mathbf{x_n})$

$ \ \ \ \ \ \ \ \ \ \ \ \ = \mathbf{w_f^T} \mathbf{w_{T-1}} + y_n \mathbf{w_f^T} \mathbf{x_n} $

$ \ \ \ \ \ \ \ \ \ \ \ \ \ge \mathbf{w_f^T} \mathbf{w_{T-1}} + \min_n y_n \mathbf{w_f^T} \mathbf{x_n} $

$ \ \ \ \ \ \ \ \ \ \ \ \ = \mathbf{w_f^T} \mathbf{w_0} + T\min_n y_n \mathbf{w_f^T} \mathbf{x_n} $

$ \ \ \ \ \ \ \ \ \ \ \ \ = T\min_n y_n \mathbf{w_f^T} \mathbf{x_n} $


$ ||\mathbf{w_T}||^2 = ||\mathbf{w_{T-1}} + y_{n(T-1)} \mathbf{x_{n(T-1)}} ||^2$

$ \ \ \ \ \ \ \ \ \ \ \ \ = ||\mathbf{w_{T-1}}||^2 + 2y_{n(T-1)} \mathbf{w_{T-1}^T} \mathbf{x_{n(T-1)}} + ||y_{n(T-1)} \mathbf{x_{n(T-1)}} ||^2$

$ \ \ \ \ \ \ \ \ \ \ \ \ \le ||\mathbf{w_{T-1}}||^2 +||y_{n(T-1)} \mathbf{x_{n(T-1)}} ||^2$

$ \ \ \ \ \ \ \ \ \ \ \ \ \le ||\mathbf{w_{T-1}}||^2 + \max_n||y_n \mathbf{x_n} ||^2$

$ \ \ \ \ \ \ \ \ \ \ \ \ = ||\mathbf{w_{T-1}}||^2 + \max_n|| \mathbf{x_n} ||^2$

$ \ \ \ \ \ \ \ \ \ \ \ \ = ||\mathbf{w_0}||^2 + T \max_n|| \mathbf{x_n} ||^2$

$ \ \ \ \ \ \ \ \ \ \ \ \ = T \max_n|| \mathbf{x_n} ||^2$


$ \frac {\mathbf{w_f^T}} {||\mathbf{w_f}||} \frac {\mathbf{w_T}} {||\mathbf{w_T}||} \ge \frac {T\min_n y_n \mathbf{w_f^T} \mathbf{x_n}} {||\mathbf{w_f}|| \sqrt{T \max_n|| \mathbf{x_n} ||^2}}$

$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = \frac {T\min_n y_n \mathbf{w_f^T} \mathbf{x_n}} {||\mathbf{w_f}|| \sqrt{T \max_n|| \mathbf{x_n} ||^2}}$

$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = \sqrt {T}\frac {\min_n y_n \frac {\mathbf{w_f^T}} {||\mathbf{w_f}||} \mathbf{x_n}} { \sqrt{\max_n|| \mathbf{x_n} ||^2}}$

$ 1 \ge \frac {\mathbf{w_f^T}} {||\mathbf{w_f}||} \frac {\mathbf{w_T}} {||\mathbf{w_T}||} $ $ \Longrightarrow $ $ T \le \frac {\max_n|| \mathbf{x_n}||^2} {({\min_n y_n \frac {\mathbf{w_f^T}} {||\mathbf{w_f}||} \mathbf{x_n}})^2} $

可知，$ \mathbf{w_t} $是不断缓慢接近$ \mathbf{w_f} $的，并且T是有上限的，也即在数据样本线性可分的情况下，PLA最终是会在有限的次数内找到一个接近$ f $的$ g $并停下来的。

## 2.4 None Separable Data

线性不可分的数据

PLA的缺陷：
- 'assumes' linear separable $ D $ to halt —— property unknown in advance
- not fully sure how long halting takes ($ T $ depends on $ \mathbf{w_f} $) —— though practically fast

样本中通常会混入noise，如录入错误、判断错误等，这种情况下原本线性可分的数据都会变成线性不可分，如下图。

![image](http://ow5t5k2fx.bkt.clouddn.com/2.5.png)

- assume 'little' noise: $ y_n = f(x_n) $ usually
- if so, $ g \approx f $ on $ D $ $ \Leftrightarrow $ $ y_n = g(x_n) $ usually
- try, $ \mathbf{w_g} \leftarrow \arg\min_w \sum_{n=1}^N[y_n \neq sign(\mathbf{w^T \mathbf{x_n}})]$  —— **NP-hard to solve**

近似的求解算法 —— Pocket Algorithm

modify PLA algorithm by keeping best weights in pocket

initialize pocket weights $ \mathbf{\hat w}$, for t = 0, 1, …
- find a (random) mistake of $ \mathbf{w_t} $ called $ (x_{n(t)}, y_{n(t)}) $
- (try to )correct the mistake by $ \mathbf{w_{t+1}} \leftarrow \mathbf{w_t} + y_{n(t)} \mathbf{x_{n(t)}} $ 
- if $ \mathbf{w_{t+1}} $ makes fewer mistakes than $ \mathbf{\hat w}$, replace $ \mathbf{\hat w}$ by $ \mathbf{w_{t+1}} $

…until enough iterations
return$ \mathbf{\hat w}$ (called $ \mathbf{w_{POCKET}} $) as g
